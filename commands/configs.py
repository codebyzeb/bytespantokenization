HF_USERNAME = "InfoTokenizers"

# tokenizer repository
TOK_REPO_ID = "tokenizers"
BYTELEVEL_TOK_FOLDER = "bytelevel"


# data repository
FINEWEBEDU_REPO_ID = "finewebedu-20B"
BYTE_DATA_FOLDER = "bytelevel"
BYTE_DATA_SUBSET_FOLDER = "bytelevel-subset"
BYTE_LLM_PREDICTION_DATA = "bytelevel-llm-data"

# ngram model repository
BYTE_MODELS_REPO_ID = "byte-level-models"
NGRAM_MODEL_FOLDER = "ngram"
BYTE_LLM_MODEL_FOLDER = "llm"

# ngram training
MAX_NGRAM_LENGTH = 5

# subset size for training byte-level models
NUM_TRAIN_ROWS = 100000
