import copy
import multiprocessing
import shutil
from collections.abc import Generator
from pathlib import Path
from typing import Any

import dill as pickle
import typer
from datasets import Dataset, load_dataset
from huggingface_hub import HfApi
from nltk.lm import AbsoluteDiscountingInterpolated as NGRAM_MODEL
from nltk.lm.counter import NgramCounter
from nltk.lm.preprocessing import padded_everygram_pipeline
from rich import print
from transformers import AutoTokenizer, PreTrainedTokenizerFast

from commands.configs import (
    BYTE_DATA_SUBSET_FOLDER,
    BYTE_MODELS_REPO_ID,
    BYTELEVEL_TOK_FOLDER,
    FINEWEBEDU_REPO_ID,
    HF_USERNAME,
    MAX_NGRAM_LENGTH,
    NGRAM_MODEL_FOLDER,
    NUM_TRAIN_ROWS,
    TOK_REPO_ID,
)

app = typer.Typer()


def _update_counter_with_counter(counter1: NgramCounter, counter2: NgramCounter) -> None:
    """
    Add two Counter objects together, merging their counts.
    """
    # Update unigram counts
    for key, value in counter2[1].items():
        if key not in counter1[1]:
            counter1[1][key] = value
        else:
            counter1[1][key] += value

    # Update conditional frequency counts
    for order in range(2, len(counter1) + 1):
        for context, freqs in counter2[order].items():
            if context not in counter1[order]:
                counter1[order][context] = freqs.copy()
            else:
                for word, freq in freqs.items():
                    if word not in counter1[order][context]:
                        counter1[order][context][word] = freq
                    else:
                        counter1[order][context][word] += freq


def _combine_ngram_models(models) -> Any:
    """
    Combine multiple Kneser-Ney models into a single model.

    :param models: A list of ngram models to combine.
    :return: A combined ngram model.
    """
    # Make a copy of the first model
    # to avoid modifying the original model
    combined_model = copy.deepcopy(models[0])

    # Update vocabulary
    for model in models[1:]:
        for word in model.vocab:
            if word not in combined_model.vocab:
                combined_model.vocab.counts[word] = model.vocab[word]
            else:
                combined_model.vocab.counts[word] += model.vocab[word]

    # Update the counts
    for model in models[1:]:
        _update_counter_with_counter(combined_model.counts, model.counts)

    return combined_model


def _train(data_chunk):
    train_data, padded_sents = padded_everygram_pipeline(order=MAX_NGRAM_LENGTH, text=data_chunk)
    model = NGRAM_MODEL(order=MAX_NGRAM_LENGTH)
    model.fit(train_data, padded_sents)

    return model


class TokenIterable:
    def __init__(self, dataset: Dataset, tokenizer: PreTrainedTokenizerFast) -> None:
        self.dataset = dataset
        self.tokenizer = tokenizer

    def __iter__(self) -> Generator[str | list[str], Any, None]:
        for sample in self.dataset:
            yield self.tokenizer.convert_ids_to_tokens(sample["input_ids"])  # type: ignore


@app.command()
def train_ngram_model() -> None:
    data_path = f"{HF_USERNAME}/{FINEWEBEDU_REPO_ID}"
    tokenizer_path = f"{HF_USERNAME}/{TOK_REPO_ID}"
    folder_path = Path(f"{BYTE_MODELS_REPO_ID}/{NGRAM_MODEL_FOLDER}")
    folder_path.mkdir(parents=True, exist_ok=True)
    print(f"üí° Will save the ngram model locally to to: {folder_path}")

    # Load tokenizer data
    tokenizer = AutoTokenizer.from_pretrained(
        pretrained_model_name_or_path=tokenizer_path, subfolder=BYTELEVEL_TOK_FOLDER
    )
    dataset = load_dataset(data_path, name=BYTE_DATA_SUBSET_FOLDER, split="train", streaming=True)
    dataset = dataset.select_columns("input_ids")

    # Split into chunks for parallel processing
    num_chunks = multiprocessing.cpu_count()
    print(f"üí° Splitting data into {num_chunks} shards for training ngram LMs")
    iterables = []
    for i in range(num_chunks):
        iterables.append(
            TokenIterable(dataset.shard(num_chunks, index=i).take(NUM_TRAIN_ROWS // num_chunks), tokenizer)  # type: ignore
        )

    print("‚öôÔ∏è Training the KneserNeyInterpolated ngram models (this may take a while)...")
    with multiprocessing.Pool(num_chunks) as pool:
        models = pool.map(_train, iterables)
    model = _combine_ngram_models(models)
    print("‚úÖ Successfully trained ngram models")

    with open(folder_path / f"{MAX_NGRAM_LENGTH}-gram-model.pkl", "wb") as fout:
        pickle.dump(model, fout)

    repo_id = f"{HF_USERNAME}/{folder_path.parent}"
    print(f"üÜô Uploading the model to {repo_id} on the HF Hub")

    api = HfApi()
    api.create_repo(repo_id, exist_ok=True)
    api.upload_folder(
        folder_path=folder_path, repo_id=repo_id, path_in_repo=folder_path.name, repo_type="model", revision="main"
    )

    print(f"‚úÖ Successfully created and uploaded the ngram model to {repo_id}")

    shutil.rmtree(folder_path, ignore_errors=True)


if __name__ == "__main__":
    app()
