{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Byte-Level Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers, processors\n",
    "from transformers import PreTrainedTokenizerFast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/InfoTokenizers/Byte-Level-Tokenizer/commit/1a6c859d97210bb52a99a3aef8cfd7539b60ac63', commit_message='Upload tokenizer', commit_description='', oid='1a6c859d97210bb52a99a3aef8cfd7539b60ac63', pr_url='https://huggingface.co/InfoTokenizers/Byte-Level-Tokenizer/discussions/1', repo_url=RepoUrl('https://huggingface.co/InfoTokenizers/Byte-Level-Tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='InfoTokenizers/Byte-Level-Tokenizer'), pr_revision='refs/pr/1', pr_num=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_prefix_space = True  # Note that we will add a prefix_space to the pre_tokenizer\n",
    "PAD_TOKEN = \"<|padding|>\"\n",
    "EOS_TOKEN = \"<|endoftext|>\"\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.NFD()])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space, use_regex=True)\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)  # type: ignore\n",
    "tokenizer.decoder = decoders.ByteLevel()  # type: ignore\n",
    "\n",
    "# \"Train\", i.e., add the properties that we need\n",
    "trainer = trainers.BpeTrainer(special_tokens=[PAD_TOKEN, EOS_TOKEN], initial_alphabet=pre_tokenizers.ByteLevel.alphabet())\n",
    "tokenizer.train_from_iterator([], trainer=trainer)\n",
    "\n",
    "# Load the tokenizer as a transformers-compatible tokenizer\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer, pad_token=PAD_TOKEN, unk_token=None, bos_token=None, eos_token=EOS_TOKEN, add_prefix_space=add_prefix_space)\n",
    "wrapped_tokenizer.push_to_hub(\"InfoTokenizers/Byte-Level-Tokenizer\", create_pr=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ġ', 'H', 'e', 'l', 'l', 'o', ',', 'Ġ', 'm', 'y', 'Ġ', 'n', 'a', 'm', 'e', 'Ġ', 'i', 's', 'Ġ', 'Z', 'e', 'Ì', 'ģ', 'b', 'u', 'l', 'o', 'n', '.']\n",
      "[222, 41, 70, 77, 77, 80, 13, 222, 78, 90, 222, 79, 66, 78, 70, 222, 74, 84, 222, 59, 70, 138, 225, 67, 86, 77, 80, 79, 15]\n",
      "['Ġ', 'H', 'e', 'l', 'l', 'o', ',', 'Ġ', 'm', 'y', 'Ġ', 'n', 'a', 'm', 'e', 'Ġ', 'i', 's', 'Ġ', 'Z', 'e', 'Ì', 'ģ', 'b', 'u', 'l', 'o', 'n', '.']\n",
      "-Hello,-my-name-is-Zébulon.\n"
     ]
    }
   ],
   "source": [
    "example = \"Hello, my name is Zébulon.\"\n",
    "t = tokenizer.encode(example)\n",
    "print(t.tokens)\n",
    "print(t.ids)\n",
    "print([ tokenizer.id_to_token(id) for id in t.ids ])\n",
    "print(tokenizer.decode(t.ids, skip_special_tokens=False).replace(\" \", \"-\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_byte_tokenizer():\n",
    "    tokenizer = Tokenizer(models.BPE())\n",
    "    tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Strip()])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True, use_regex=True)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "    trainer = trainers.BpeTrainer(special_tokens=[\"<|padding|>\", \"<|endoftext|>\"], vocab_size=258, initial_alphabet=pre_tokenizers.ByteLevel.alphabet())\n",
    "    tokenizer.train_from_iterator([], trainer=trainer, length=0)\n",
    "\n",
    "    vocab = pre_tokenizers.ByteLevel.alphabet()\n",
    "    vocab.append('<|padding|>')\n",
    "    vocab.append('<|endoftext|>')\n",
    "    # Add special bpe space symbol\n",
    "    tokenizer.add_tokens(vocab)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "['H', 'e', 'l', 'l', 'o', ',', 'Ġ', 'm', 'y', 'Ġ', 'n', 'a', 'm', 'e', 'Ġ', 'i', 's', 'Ġ', 'Z', 'é', 'b', 'u', 'l', 'o', 'n', '.']\n",
      "[41, 70, 77, 77, 80, 13, 222, 78, 90, 222, 79, 66, 78, 70, 222, 74, 84, 222, 59, 167, 67, 86, 77, 80, 79, 15]\n",
      "['H', 'e', 'l', 'l', 'o', ',', 'Ġ', 'm', 'y', 'Ġ', 'n', 'a', 'm', 'e', 'Ġ', 'i', 's', 'Ġ', 'Z', 'é', 'b', 'u', 'l', 'o', 'n', '.']\n",
      "Hello, my name is Z�bulon.\n"
     ]
    }
   ],
   "source": [
    "example = \"Hello, my name is Zébulon.\"\n",
    "#example = \"I can 嗎 feel ₉the magic, can you?\"\n",
    "tokenizer = get_byte_tokenizer()\n",
    "t = tokenizer.encode(example)\n",
    "print(t.tokens)\n",
    "print(t.ids)\n",
    "print([ tokenizer.id_to_token(id) for id in t.ids ])\n",
    "print(tokenizer.decode(t.ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/InfoTokenizers/Byte-Level-Tokenizer/commit/d0c52a16e43cf4542fb4207b561fe8e88f0a3aab', commit_message='Upload tokenizer', commit_description='', oid='d0c52a16e43cf4542fb4207b561fe8e88f0a3aab', pr_url=None, repo_url=RepoUrl('https://huggingface.co/InfoTokenizers/Byte-Level-Tokenizer', endpoint='https://huggingface.co', repo_type='model', repo_id='InfoTokenizers/Byte-Level-Tokenizer'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer, pad_token='<|padding|>', unk_token=None, bos_token=None, eos_token='<|endoftext|>', add_prefix_space=True)\n",
    "wrapped_tokenizer.push_to_hub(\"InfoTokenizers/Byte-Level-Tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
