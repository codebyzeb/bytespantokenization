{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f42ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_eval(modelfile, subfolder, evaldata, outdir):\n",
    "    from transformers import AutoTokenizer\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        modelfile,\n",
    "        subfolder=subfolder\n",
    "    )\n",
    "\n",
    "    lexdata = pd.read_csv(open(evaldata, \"r\"), delimiter=\"\\t\")\n",
    "    tokens = [str(x) for x in lexdata[\"spelling\"]]\n",
    "    lexicality = lexdata[\"lexicality\"]\n",
    "\n",
    "    subtokens = [tokenizer.tokenize(token) for token in tokens]\n",
    "\n",
    "    chunkability = [1 - (len(subtokens[i]) / len(tokens[i])) for i in range(len(tokens))]\n",
    "    num_splits = [len(toklist) - 1 for toklist in subtokens]\n",
    "    reading_times = lexdata[\"rt\"]\n",
    "    lengths = [len(token) for token in tokens]\n",
    "    accuracies = lexdata[\"accuracy\"]\n",
    "\n",
    "    results = pd.DataFrame(list(\n",
    "        zip(tokens, lexicality, lengths, subtokens, num_splits, chunkability, reading_times, accuracies)\n",
    "    ), columns=[\"Stimulus\", \"Lexicality\", \"Length\", \"Subtokens\", \"Num_Splits\", \"Chunkability\", \"Reading_Time\", \"Accuracy\"])\n",
    "\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    outfile = os.path.join(outdir, f\"{subfolder}_output.csv\")\n",
    "    results.to_csv(outfile, index=False)\n",
    "    print(f\"Saved results for {subfolder} to {outfile}\")\n",
    "\n",
    "\n",
    "\n",
    "# --- TOKENIZER SUBFOLDERS ---\n",
    "subfolders = [\n",
    "    \"bytelevel\",\n",
    "    \"frequency_128000\",\n",
    "    \"frequency_16000\",\n",
    "    \"frequency_256000\",\n",
    "    \"frequency_32000\",\n",
    "    \"frequency_64000\",\n",
    "    \"frequency_8064\",\n",
    "    \"fw57M_Entropy_frequency-mean-post-merge_128000\",\n",
    "    \"fw57M_Entropy_frequency-mean-post-merge_16000\",\n",
    "    \"fw57M_Entropy_frequency-mean-post-merge_256000\",\n",
    "    \"fw57M_Entropy_frequency-mean-post-merge_32000\",\n",
    "    \"fw57M_Entropy_frequency-mean-post-merge_64000\",\n",
    "    \"fw57M_Entropy_frequency-mean-post-merge_8064\",\n",
    "    \"fw57M_Entropy_min-mean-post-merge_128000\",\n",
    "    \"fw57M_Entropy_min-mean-post-merge_16000\",\n",
    "    \"fw57M_Entropy_min-mean-post-merge_256000\",\n",
    "    \"fw57M_Entropy_min-mean-post-merge_32000\",\n",
    "    \"fw57M_Entropy_min-mean-post-merge_64000\",\n",
    "    \"fw57M_Entropy_min-mean-post-merge_8064\",\n",
    "    \"fw57M_Surprisal_frequency-mean-post-merge_128000\",\n",
    "    \"fw57M_Surprisal_frequency-mean-post-merge_16000\",\n",
    "    \"fw57M_Surprisal_frequency-mean-post-merge_256000\",\n",
    "    \"fw57M_Surprisal_frequency-mean-post-merge_32000\",\n",
    "    \"fw57M_Surprisal_frequency-mean-post-merge_64000\",\n",
    "    \"fw57M_Surprisal_frequency-mean-post-merge_8064\",\n",
    "    \"fw57M_Surprisal_min-mean-post-merge_128000\",\n",
    "    \"fw57M_Surprisal_min-mean-post-merge_16000\",\n",
    "    \"fw57M_Surprisal_min-mean-post-merge_256000\",\n",
    "    \"fw57M_Surprisal_min-mean-post-merge_32000\",\n",
    "    \"fw57M_Surprisal_min-mean-post-merge_64000\",\n",
    "    \"fw57M_Surprisal_min-mean-post-merge_8064\",\n",
    "    \"ngram_Entropy_frequency-mean-post-merge_128000\",\n",
    "    \"ngram_Entropy_frequency-mean-post-merge_16000\",\n",
    "    \"ngram_Entropy_frequency-mean-post-merge_256000\",\n",
    "    \"ngram_Entropy_frequency-mean-post-merge_32000\",\n",
    "    \"ngram_Entropy_frequency-mean-post-merge_64000\",\n",
    "    \"ngram_Entropy_frequency-mean-post-merge_8064\",\n",
    "    \"ngram_Entropy_min-mean-post-merge_128000\",\n",
    "    \"ngram_Entropy_min-mean-post-merge_16000\",\n",
    "    \"ngram_Entropy_min-mean-post-merge_256000\",\n",
    "    \"ngram_Entropy_min-mean-post-merge_32000\",\n",
    "    \"ngram_Entropy_min-mean-post-merge_64000\",\n",
    "    \"ngram_Entropy_min-mean-post-merge_8064\",\n",
    "    \"ngram_Space Probability_frequency-mean-post-merge_128000\",\n",
    "    \"ngram_Space Probability_frequency-mean-post-merge_16000\",\n",
    "    \"ngram_Space Probability_frequency-mean-post-merge_256000\",\n",
    "    \"ngram_Space Probability_frequency-mean-post-merge_32000\",\n",
    "    \"ngram_Space Probability_frequency-mean-post-merge_64000\",\n",
    "    \"ngram_Space Probability_frequency-mean-post-merge_8064\",\n",
    "    \"ngram_Space Probability_min-mean-post-merge_128000\",\n",
    "    \"ngram_Space Probability_min-mean-post-merge_16000\",\n",
    "    \"ngram_Space Probability_min-mean-post-merge_256000\",\n",
    "    \"ngram_Space Probability_min-mean-post-merge_32000\",\n",
    "    \"ngram_Space Probability_min-mean-post-merge_64000\",\n",
    "    \"ngram_Space Probability_min-mean-post-merge_8064\",\n",
    "    \"ngram_Surprisal_frequency-mean-post-merge_128000\",\n",
    "    \"ngram_Surprisal_frequency-mean-post-merge_16000\",\n",
    "    \"ngram_Surprisal_frequency-mean-post-merge_256000\",\n",
    "    \"ngram_Surprisal_frequency-mean-post-merge_32000\",\n",
    "    \"ngram_Surprisal_frequency-mean-post-merge_64000\",\n",
    "    \"ngram_Surprisal_frequency-mean-post-merge_8064\",\n",
    "    \"ngram_Surprisal_min-mean-post-merge_128000\",\n",
    "    \"ngram_Surprisal_min-mean-post-merge_16000\",\n",
    "    \"ngram_Surprisal_min-mean-post-merge_256000\",\n",
    "    \"ngram_Surprisal_min-mean-post-merge_32000\",\n",
    "    \"ngram_Surprisal_min-mean-post-merge_64000\",\n",
    "    \"ngram_Surprisal_min-mean-post-merge_8064\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1e1638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SETTINGS ---\n",
    "modelfile = \"InfoTokenizers/tokenizers\"\n",
    "\n",
    "# Set a local path to blp-items.txt\n",
    "evaldata = #download this from https://github.com/codebyzeb/infotokenization/blob/main/eval/cog-plausibility/en/blp-items.txt\n",
    "outdir = #some output directory, e.g., \"/Tokenizers/chunk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6856d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- RUN ---\n",
    "for subfolder in subfolders:\n",
    "    tokenize_eval(modelfile, subfolder, evaldata, outdir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a784bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "#we load directory containing chunk stats\n",
    "chunk_stats = []\n",
    "\n",
    "for file in files:\n",
    "    # Only include relevant tokenizer outputs\n",
    "    if not file.endswith(\"_output.csv\") and not file.endswith(\"_output.tsv\"):\n",
    "        continue\n",
    "\n",
    "    filepath = os.path.join(outdir, file)\n",
    "    try:\n",
    "        if file.endswith(\".tsv\"):\n",
    "            df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "        else:\n",
    "            df = pd.read_csv(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Check that all required columns exist\n",
    "    required_columns = {\"Lexicality\", \"Chunkability\", \"Num_Splits\"}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        print(f\"Skipping {file}: missing columns {required_columns - set(df.columns)}\")\n",
    "        continue\n",
    "\n",
    "    for category_label, category_df in df.groupby(\"Lexicality\"):\n",
    "        label = \"Words\" if category_label == \"W\" else \"Non-Words\"\n",
    "\n",
    "        chunkability_vals = category_df[\"Chunkability\"].dropna()\n",
    "        num_splits_vals = category_df[\"Num_Splits\"].dropna()\n",
    "\n",
    "        chunk_stats.append({\n",
    "            \"Tokenizer\": file.replace(\".csv\", \"\").replace(\".tsv\", \"\"),\n",
    "            \"Category\": label,\n",
    "            \"Chunkability_Mean\": np.mean(chunkability_vals),\n",
    "            \"Chunkability_Stdev\": np.std(chunkability_vals),\n",
    "            \"NumSplits_Mean\": np.mean(num_splits_vals),\n",
    "            \"NumSplits_Stdev\": np.std(num_splits_vals)\n",
    "        })\n",
    "\n",
    "# Save results\n",
    "chunk_stats_df = pd.DataFrame(chunk_stats)\n",
    "chunk_path = os.path.join(outdir, \"chunkability_stats.tsv\")\n",
    "chunk_stats_df.to_csv(chunk_path, sep=\"\\t\", index=False)\n",
    "print(f\"\\nSaved chunkability stats to {chunk_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf4fa91",
   "metadata": {},
   "source": [
    "Ok, so to get the table you can use either of the two cells and should get the same output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00c7cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from ast import literal_eval\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Collect all CSV files\n",
    "files = [f for f in os.listdir(outdir) if f.endswith(\".csv\") or f.endswith(\".tsv\")]\n",
    "print(f\"Found {len(files)} tokenizer output files.\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Checking for non-significant correlations at p >= 0.01\\n\")\n",
    "\n",
    "for file in files:\n",
    "    # Only include relevant tokenizer outputs\n",
    "    if not file.endswith(\"_output.csv\") and not file.endswith(\"_output.tsv\"):\n",
    "        continue # Skip files that are not tokenizer outputs\n",
    "\n",
    "    filepath = os.path.join(outdir, file)\n",
    "    print(f\"Tokenizer: {file}\")\n",
    "\n",
    "    # Load tokenizer output\n",
    "    if file.endswith(\".tsv\"):\n",
    "        df = pd.read_csv(filepath, sep=\"\\t\")\n",
    "    else:\n",
    "        df = pd.read_csv(filepath)\n",
    "\n",
    "    # Split into words and nonwords\n",
    "    words = df[df[\"Lexicality\"] == \"W\"]\n",
    "    nonwords = df[df[\"Lexicality\"] == \"N\"]\n",
    "\n",
    "    datasets = {\"words\": words, \"nonwords\": nonwords}\n",
    "\n",
    "    # Iterate through categories\n",
    "    for category, dataset in datasets.items():\n",
    "        tokens = list(dataset[\"Stimulus\"])\n",
    "        rts = list(dataset[\"Reading_Time\"])\n",
    "        accs = list(dataset[\"Accuracy\"])\n",
    "\n",
    "        # Get number of splits and wordiness\n",
    "        num_splits = list(dataset[\"Num_Splits\"])\n",
    "        wordiness = list(dataset[\"Chunkability\"])\n",
    "\n",
    "        # Compute correlations\n",
    "        corr1, p1 = pearsonr(num_splits, rts)\n",
    "        corr2, p2 = pearsonr(num_splits, accs)\n",
    "        corr3, p3 = pearsonr(wordiness, rts)\n",
    "        corr4, p4 = pearsonr(wordiness, accs)\n",
    "\n",
    "        results.append({\n",
    "            \"tokenizer\": file.replace(\".csv\", \"\").replace(\".tsv\", \"\"),\n",
    "            \"category\": category,\n",
    "            \"measure\": \"NumSplits vs RT\",\n",
    "            \"r\": corr1,\n",
    "            \"p\": p1\n",
    "        })\n",
    "        results.append({\n",
    "            \"tokenizer\": file.replace(\".csv\", \"\").replace(\".tsv\", \"\"),\n",
    "            \"category\": category,\n",
    "            \"measure\": \"NumSplits vs Acc\",\n",
    "            \"r\": corr2,\n",
    "            \"p\": p2\n",
    "        })\n",
    "        results.append({\n",
    "            \"tokenizer\": file.replace(\".csv\", \"\").replace(\".tsv\", \"\"),\n",
    "            \"category\": category,\n",
    "            \"measure\": \"Wordiness vs RT\",\n",
    "            \"r\": corr3,\n",
    "            \"p\": p3\n",
    "        })\n",
    "        results.append({\n",
    "            \"tokenizer\": file.replace(\".csv\", \"\").replace(\".tsv\", \"\"),\n",
    "            \"category\": category,\n",
    "            \"measure\": \"Wordiness vs Acc\",\n",
    "            \"r\": corr4,\n",
    "            \"p\": p4\n",
    "        })\n",
    "\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# Save all results to a CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "save_path = os.path.join(outdir, \"correlation_results.tsv\")\n",
    "results_df.to_csv(save_path, sep=\"\\t\", index=False)\n",
    "\n",
    "print(f\"\\nSaved all correlation results to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481bc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from ast import literal_eval\n",
    "import os\n",
    "# Directories for evaluation and results\n",
    "save_path = os.path.join(outdir, \"final_correlation_results.csv\")\n",
    "\n",
    "# Function to evaluate the correlation results from the CSV files\n",
    "def evaluate_correlation_results():\n",
    "    # Define the path where the tokenizer results are stored\n",
    "    resultpath = outdir_tokenizer  # Using the same directory as the output folder\n",
    "    files = os.listdir(resultpath)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for file in files:\n",
    "        # Process only CSV files and filter out the correlation results files\n",
    "        if file.endswith(\".csv\") and not file.startswith(\"correlation_results\") and not file.startswith(\"chunkability_stats\") and not file.startswith(\"final_correlation_results\"):\n",
    "            filepath = os.path.join(resultpath, file)\n",
    "            print(f\"Tokenizer: {file}\")\n",
    "            \n",
    "            # Load CSV file\n",
    "            df = pd.read_csv(filepath)  # Read CSV files\n",
    "            \n",
    "            # Drop rows with missing values\n",
    "            df = df.dropna()\n",
    "\n",
    "            # Split the data into words and nonwords\n",
    "            words = df[df[\"Lexicality\"] == \"W\"]\n",
    "            nonwords = df[df[\"Lexicality\"] == \"N\"]\n",
    "            datasets = {\"words\": words, \"nonwords\": nonwords}\n",
    "            \n",
    "            print(f\"Processing file: {file}\")\n",
    "\n",
    "            # Extract filename without extension\n",
    "            filename_no_extension = os.path.splitext(file)[0]\n",
    "\n",
    "            # Iterate through categories (words, nonwords)\n",
    "            for category, dataset in datasets.items():\n",
    "                print(f\"Processing category: {category}\")\n",
    "                \n",
    "                # Initialize variables to store correlations\n",
    "                category_results = []\n",
    "\n",
    "                # Measurements\n",
    "                tokens = list(dataset[\"Stimulus\"])\n",
    "                rts = list(dataset[\"Reading_Time\"])\n",
    "                accs = list(dataset[\"Accuracy\"])\n",
    "                num_splits = list(dataset[\"Num_Splits\"])\n",
    "                chunkability = list(dataset[\"Chunkability\"])\n",
    "\n",
    "                # Calculate splits and wordiness for each model\n",
    "                splits = list(dataset[\"Subtokens\"].apply(literal_eval))  # Assuming 'Subtokens' contains the splits\n",
    "                num_splits = [len(x) - 1 for x in splits]  # Number of splits is len(splits) - 1\n",
    "                max_len = len(dataset)\n",
    "                wordiness = [1 - (len(splits[i]) / len(str(tokens[i]))) for i in range(max_len)]  # Wordiness formula\n",
    "\n",
    "                # Compute correlations\n",
    "                corr1, p1 = pearsonr(num_splits, rts)   # Correlation of Num_Splits with RT\n",
    "                corr2, p2 = pearsonr(num_splits, accs)  # Correlation of Num_Splits with Accuracy\n",
    "                corr3, p3 = pearsonr(wordiness, rts)    # Correlation of Wordiness with RT\n",
    "                corr4, p4 = pearsonr(wordiness, accs)   # Correlation of Wordiness with Accuracy\n",
    "\n",
    "                # Store the results for the current category\n",
    "                results = [\n",
    "                    filename_no_extension,  # Add the filename (without extension)\n",
    "                    category,\n",
    "                    \"{:.2f}\".format(corr1),\n",
    "                    \"{:.2f}\".format(corr2),\n",
    "                    \"{:.2f}\".format(corr3),\n",
    "                    \"{:.2f}\".format(corr4)\n",
    "                ]\n",
    "                category_results.append(results)\n",
    "                \n",
    "                # Append the results for the category\n",
    "                all_results.append((category, category_results))\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "        # Write the header\n",
    "        outfile.write(\"Tokenizer, Category, NumSplits_RT, NumSplits_Acc, Chunkability_RT, Chunkability_Acc\\n\")\n",
    "        \n",
    "        # Write each result line\n",
    "        for category, category_results in all_results:\n",
    "            for result in category_results:\n",
    "                filename, category, result1, result2, result3, result4 = result\n",
    "                # Write result for each category\n",
    "                outfile.write(f\"{filename}, {category}, {result1}, {result2}, {result3}, {result4}\\n\")\n",
    "\n",
    "    print(f\"Correlation results saved to {save_path}\")\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_correlation_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
